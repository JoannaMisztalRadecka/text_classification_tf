{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDfkjh5g1PV8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/JoannaMisztalRadecka/text_classification_tf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHOUKAtH2F6S"
      },
      "outputs": [],
      "source": [
        "! pip install -r text_classification_tf/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBqge0KkpSre"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VxtpSRC11oo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf \n",
        "\n",
        "import sys\n",
        "sys.path.append('text_classification_tf')\n",
        "\n",
        "from text_classification.hypermodel import StandardTextClassificationHyperModel, \\\n",
        "TFHubEmbeddingTextClassificationHyperModel, BertTextClassificationHyperModel,\\\n",
        "get_best_model\n",
        "\n",
        "from text_classification.dataset import get_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO4w7sQ51zrb"
      },
      "outputs": [],
      "source": [
        "loss = 'sparse_categorical_crossentropy'\n",
        "metric = 'accuracy'\n",
        "objective=f\"val_{metric}\"\n",
        "n_output_units = 20\n",
        "max_trials = 5\n",
        "executions_per_trial = 1\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "seed = 123\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset_dir = \"aclImdb_v1\"\n",
        "train_dir = \"aclImdb/train\"\n",
        "\n",
        "# url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
        "# dataset_dir = \"stack_overflow_16k\"\n",
        "# train_dir = \"train\"\n",
        "\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeZu9Z1x19hV"
      },
      "outputs": [],
      "source": [
        "# train_ds, val_ds = get_dataset(url, dataset_dir, train_dir, batch_size, seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.keras.utils.get_file(dataset_dir, url,\n",
        "                                      untar=True, cache_dir='..',\n",
        "                                      cache_subdir='')\n",
        "train_dataset_dir = os.path.join(os.path.dirname(dataset), train_dir)\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dataset_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dataset_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "id": "5yl6-inEDXFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train_dataset = fetch_20newsgroups(subset='train')\n",
        "val_dataset = fetch_20newsgroups(subset='test')"
      ],
      "metadata": {
        "id": "9dVIs_S1FLsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "aI5LGIykGot0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((np.array(train_dataset.data).astype(str), np.array(train_dataset.target).astype(int)))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((np.array(val_dataset.data).astype(str), np.array(val_dataset.target).astype(int)))"
      ],
      "metadata": {
        "id": "wA3AJlYdFpDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "dffX14J5DxvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMI8uCt8ktEc"
      },
      "source": [
        "## Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaYuIaca7fLT"
      },
      "outputs": [],
      "source": [
        "hypermodel_baseline = StandardTextClassificationHyperModel(train_ds, n_output_units, \n",
        "                                                  loss, metric)\n",
        "log_dir_baseline = os.path.join('logs', 'baseline', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "%tensorboard --logdir  $log_dir_baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOMwaDytpL0j"
      },
      "outputs": [],
      "source": [
        "best_model_baseline = get_best_model(hypermodel_baseline, log_dir_baseline, objective, train_ds, \n",
        "                            val_ds, epochs, max_trials, executions_per_trial)\n",
        "results.append({\"model\": \"baseline\", objective: best_model_baseline.evaluate(val_ds)[1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1j0b2WQp-Cu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhuw2x1Xk_ou"
      },
      "source": [
        "## Model with pre-computed text embeddings from TF-Hub (transfer learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnEssfhrSOW2"
      },
      "outputs": [],
      "source": [
        "hypermodel_tf_hub = TFHubEmbeddingTextClassificationHyperModel(train_ds, n_output_units, \n",
        "                                                  loss, metric)\n",
        "\n",
        "log_dir_tf_hub = os.path.join('logs', 'tf_hub', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir  $log_dir_tf_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz_IaTNxo0cA"
      },
      "outputs": [],
      "source": [
        "best_model_tf_hub = get_best_model(hypermodel_tf_hub, log_dir_tf_hub, objective, train_ds, \n",
        "                            val_ds, epochs, max_trials, executions_per_trial)\n",
        "results.append({\"model\": \"tf_hub\", objective: best_model_tf_hub.evaluate(val_ds)[1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_gXYImdlwjV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLmYf_S0lxZn"
      },
      "source": [
        "## Model with pre-computed Bert embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf3DC029k4qm"
      },
      "outputs": [],
      "source": [
        "hypermodel_bert = BertTextClassificationHyperModel(train_ds, n_output_units, \n",
        "                                                   loss, metric)\n",
        "log_dir_bert = os.path.join('logs', 'bert', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir  $log_dir_bert\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS1dxOJEq_a-"
      },
      "outputs": [],
      "source": [
        "import tensorflow_text # required for importing Bert\n",
        "\n",
        "best_model_bert = get_best_model(hypermodel_bert, log_dir_bert, objective, train_ds, \n",
        "                            val_ds, epochs, max_trials, executions_per_trial)\n",
        "results.append({\"model\": \"bert\", objective: best_model_bert.evaluate(val_ds)[1]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BwbKynQrLV3"
      },
      "source": [
        "## Results comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSfd0uZSqzmm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1SWRKTbq04I"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text classification TF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}